---
title: "Random Forest Vignette"
format:
  html:
    toc: true
    toc_float: true
editor: visual
date: today
author: Mindy Xu, Zoe Zhou, Amy Lyu, Jiashu Huang
---

## Introduction

> If you are new to this topic, then this vignette will be a great guideline for you to gain a thorough understanding of the Random Forest model. We will also demonstrate sample code using a dataset to predict hotel cancellations with Random Forest.

*Learning Objectives:*

-   Learn the intuitions behind Random Forest models

    -   Decision trees, hyper-parameters, and more

-   Implement, tune, and evaluate Random Forest models in R

Here we have a introductory video for you to better know what is Random Forest.

[Introductory Video](https://www.youtube.com/watch?v=v6VJ2RO66Ag)

## Example Data -- Hotel Reservation Data to Implement

### Data Description

This dataset was obtained from Kaggle and contains 19 predictors. The target variable, booking status, indicates whether the reservation was canceled or not (binary: 2 values). There are a total of 36,275 observations. The goal of predicting hotel reservations is to forecast the number of bookings a hotel will receive for a specific date range, based on historical data and other relevant factors, such as seasonality, market trends, and pricing strategies.

<details>

<summary>Data Dictionary</summary>

-   Booking_ID: Unique identifier of each booking

-   no_of_adults: Number of adults

-   no_of_children: Number of Children

-   no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel

-   no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel

-   type_of_meal_plan: Type of meal plan booked by the customer

-   required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)

-   room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.

-   lead_time: Number of days between the date of booking and the arrival date

-   arrival_year: Year of the arrival date

-   arrival_month: Month of the arrival date

-   arrival_date: Date of the month

-   market_segment_type: Market segment designation.

-   repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)

-   no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking

-   no_of_previous_bookings_not_canceled: Number of previous bookings that were not canceled by the customer prior to the current booking

-   avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)

-   no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)

-   booking_status: Flag indicating if the booking was canceled or not.

</details>

### Load Packages and Data

```{r results='hide', message=FALSE, warning = FALSE}
# load packages
library(tidyverse)
library(dplyr)
library(tidymodels)
library(janitor) ## clean_names()
library(ranger) ## random forest model engine
library(ggplot2)
library(corrplot)
library(ggbeeswarm)
library(patchwork)
library(vip)

# load data
hotel <- read.csv("data/HotelReservations.csv")
```

### Preprocess the data

Before implementing the model, we need to make sure the dataset is ready for use. First, change the variable names to a unified format. They will contain only "\_", numbers, and letters.

```{r}
# cleaning predictor names
hotel <- clean_names(hotel)
```

Now let's take a look at how our data is stored.

```{r}
# view data structure and display the first 8 variables
str(hotel, vec.len = 2, list.len = 8)
```

Notice how the categorical variables are stored as character variables. We need to transform them into factors before encoding them into binary variables later..

::: callout-important
#### Action

Use `lapply()` to turn categorical variables into factors.
:::

```{r, echo=FALSE}
# Convert characters to factors and rename factor levels
hotel<- data.frame(lapply(hotel, function(x) if(is.character(x)) factor(x) else x))
```

### Exploratory Data Analysis

After proper transformations, let's explore our data a little before we move on to the models.

First, check the correlation between numerical variables.

```{r, fig.width=5, fig.height=5}
hotel %>%
  select(is.numeric) %>%
  cor() %>%
  corrplot(type = "lower", tl.cex = 0.5, tl.srt = 45)
```

`no_of_repeated_guest` is positively correlated with `no_of_previous_bookings_not_canceled`. `Arrival year` is negatively correlated with `arrival_month`. As most predictors do not overly correlate with each other, they likely contain different information. Therefore, we will use all of them as predictors for the decision tree and random forest models to draw from later.

Next, make a histogram to see the distribution of the `booking status`.

```{r, fig.width = 5, fig.height = 5}
hotel %>%
  ggplot(aes(x=booking_status)) +
  geom_bar() +
  labs(title = "Distribution of Booking Status")
```

From the plot, we can see that "Canceled" significantly outnumbers "Not_Canceled." To address this imbalance, we may use stratified sampling during data partitioning and a class_weight parameter during the model fitting process. These approaches will be illustrated in later sections.

Lastly, let's take a look at the correlation between our predictors and the target variable.

```{r}
numeric_vars <- names(hotel)[sapply(hotel, is.numeric)]  # Get names of numeric variables

plots <- list()  # List to store plots

for (var in numeric_vars) {
  p <- ggplot(hotel, aes_string(x = "booking_status", y = var)) +
    geom_quasirandom() +
    theme_minimal() +
    ggtitle(var) + 
    scale_x_discrete(labels = c("C", "NC")) +
    theme(axis.title.y = element_text(size = rel(0.6)), axis.title.x = element_text(size = rel(0.6)), axis.text.y = element_text(size = rel(0.6)), axis.text.x = element_text(size = rel(0.6)), title = element_text(size = rel(0.6)), ) 
  plots[[var]] <- p  # Store the plot in the list
  
}
```

```{r}
plots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] + plots[[5]] + plots[[6]] + plots[[7]] + plots[[8]] + plots[[9]] + plots[[10]] + plots[[11]] + plots[[12]] + plots[[13]] + plots[[14]] 
```

`lead_time` appears to be a strong predictor; `no_of_previous_booking_not_canceled` and `no_of_special_requests` also show a decent correlation with our target. We can later confirm these assumptions by inspecting feature importance in the random forest model.

### Data Partition

Split the dataset into a training and a testing set. Use "booking_status" as the stratification variable to ensure it has equivalent proportions in two sets.

```{r}
set.seed(3435)
hotel_split <- initial_split(hotel, strata=
                                 "booking_status", prop = 0.7)
hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)
```

::: callout-important
#### Action

Check the dimensions of the training and testing data to ensure that the data has been split correctly.
:::

## Preprocessing Recipe

Now let's create a recipe for our dataset.

```{r}
# Create a recipe for the dataset
hotel_recipe <- recipe(booking_status ~., data = hotel_train) 
```

Limit the amount of factor levels for each predictor using `step_other()`. Factor levels with an occurring frequency less than 0.05 would be pooled to "other".

```{r}
hotel_recipe <- hotel_recipe %>% 
  step_other(all_nominal_predictors(), threshold = 0.05)
```

Using `step_dummy()`, encode factor variables into multiple binary variables, each corresponding to a different factor level.

```{r}
hotel_recipe <- hotel_recipe  %>% 
  step_dummy(all_nominal_predictors()) 
```

## Cross-Validation Setup

K-fold cross-validation is a statistical technique that helps assess a model's ability to generalize. This method involves **partitioning** the entire dataset into **k smaller sets** and **repeatedly train/evaluate** the model on different sets, ensuring that all data have a chance to appear in both the training and evaluation sets. As a result, the assessment of the model becomes much **more robust**, as it is almost impossible for a model that overfits to perform well across all folds.

```{r}
# 10-fold Cross validation
hotel_folds <- vfold_cv(hotel_train, v = 10, strata = booking_status)
```

::: callout-important
#### Action

Try changing the number of folds for the cross-validation to see if the results vary.
:::

## Intuition and Architecture

### Understanding Decision Trees

**Decision Trees** are a fundamental component of many machine learning algorithms, known for their simplicity and interpretability. Decision trees handle **both categorical** and **numerical** data and can model complex relationships with **a series of simple decisions**.

Imagine a decision tree as a tree-like model of decisions, similar to a flowchart, where each internal node represents a test on an attribute. Each branch signifies the outcome of the test, and each leaf node denotes a class label (a decision taken after evaluating all attributes). The paths from the root to the leaf represent classification rules.

However, decision trees are prone to **overfitting**, particularly when they become overly deep and complex. A tree that perfectly models the training data might fail to generalize to new, unseen data. Therefore, balancing the depth of the tree with the amount of training data is crucial for building an effective decision tree model.

------------------------------------------------------------------------

Now let's implement a basic decision tree. `decision_tree()` defines a model using a set of if/then statements that create a tree-based structure.

\- `set_mode()` specifies the type of problem we are handling

\- `set_engine()` specifies the type of package/system to be used for fitting the model later

\- `set_args()` specifies the values of arguments of `decision_tree()`; here we set the depth of the tree to be in the range of \[1, 15\].

```{r}
# Define the model (Decision Tree)
dt_model<-decision_tree() %>% 
  set_mode("classification") %>% ## type of tasks
  set_engine("rpart") %>% ## type of engine used to fit the model
  set_args(tree_depth(c(1L, 15L))) ## Other parameters
```

Create a workflow object, set its recipe to hotel_recipe, and add dt_model as its model.

```{r}
# combine the model and the dataset to a workflow
dt_wf <- workflow() %>%
  add_recipe(hotel_recipe) %>%
  add_model(dt_model)
```

Use `fit_resamples()` to fit multiple models on the cross-validation folds we created earlier. Use "metrics =" to specify which metrics to track for evaluating each model's performance.

```{r}
# fit the model
hotel_results_dt <- fit_resamples(
  dt_wf,
  resamples = hotel_folds, ## cross-validation
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)
```

Let's see how this model performed on the training set and the testing set.

First, retrieve the training set metrics recorded during the training process. Check the variance of the metrics across different folds in order to look for signs of overfitting.

```{r}
results_summary_dt <- hotel_results_dt %>%
  collect_metrics(summarize = FALSE) %>%
  select(c(".metric", ".estimate")) %>%
  group_by(.metric) %>%
  summarise(variance = var(.estimate))
results_summary_dt
```

We see that the metrics does not vary too much across different folds. This means that the model did not overfit. Let's proceed to assess the model on the testing set.

Fit the model again, but this time on the entire training set.

```{r}
dt_fit_train <- fit(dt_wf, data = hotel_train)
```

Augment the testing set using the fitted model.

```{r}
dt_test <- augment(dt_fit_train,
                                hotel_test) %>%
  select(booking_status, starts_with(".pred"))

predictions <- dt_test %>%
  select(booking_status, .pred_class, .pred_Canceled)
```

Calculate the four metrics and present them in a tibble.

```{r}
sensitivity_val <- predictions %>% sens(truth = booking_status, estimate = .pred_class)
specificity_val <- predictions %>% spec(truth = booking_status, estimate = .pred_class)
accuracy_val <- predictions %>% accuracy(truth = booking_status, estimate = .pred_class)
roc_auc_val <- predictions %>% roc_auc(truth = booking_status, .pred_Canceled)

model_metrics <- tibble(
  .metric = c("accuracy", "roc_auc", "sensitivity", "specificity"),
  test = c(accuracy_val$.estimate, roc_auc_val$.estimate, sensitivity_val$.estimate, specificity_val$.estimate)
)
model_metrics
```

::: callout-important
#### Action

Play around with different parameters - engine, tree_depth, number of folds, etc. - and see if you can achieve a higher performance.
:::

------------------------------------------------------------------------

### Random Forest and Ensemble models

![](image/img-decisiontree.png)

The Random Forest algorithm, **an ensemble learning method** primarily used for classification and regression, constructs numerous decision trees at training time. Its fundamental concept is straightforward: it combines the predictions from multiple decision trees to produce a more accurate prediction than a single decision tree.

Each tree in a random forest is built from **bootstrap sample, drawn with replacement** from the training set. Furthermore, when constructing a tree, the split at each node is chosen from **a random subset of the features**, rather than the best split among all features. This 'bagging' strategy, combined with feature randomness, introduces diversity among the trees, leading to **more robust** overall prediction and **reducing the risk of overfitting**.

Additionally, random forests provide a measure of **feature importance**, providing insights into the predictive power of individual features in the model. The algorithm can be **parallelized** for execution, as each tree in the forest is built independently, making it well-suited for modern multi-processor computers. Moreover, it does **not require feature scaling**, such as standardization or normalization, before input, since it does not rely on distance calculations.

------------------------------------------------------------------------

Use `rand_forest()` to implement a random forest model. Similar to the decision tree model we implemented earlier, use `set_mode()` and `set_engine()` to complete our model definition.

There is a couple of parameters you can play around with:

\- `mtry`: the number of random sampled predictors used for each split

\- `trees`: number of trees

\- `min_n`: minimum number of data needed in a node to split further into two branches.

There are several methods for determining the optimal way to split when building a tree. The ranger engine allows us to specify this. We set `importance = impurity` in `set_engine()`, so that the engine used gini index to determine how to split.

```{r}
rf_model <- rand_forest(mtry = 10,
                        trees = 10, 
                        min_n = 10) %>% 
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")
```

Again, combine the model and recipe into a workflow.

```{r}
rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(hotel_recipe)
```

Fit the model using `fit_resamples()`. You will notice this time it takes longer for the model to fit.

```{r}
# fit the model
hotel_results_rf <- fit_resamples(
  rf_wf,
  resamples = hotel_folds,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)
```

Check the variance of the model metrics across different folds

```{r}
# summarize the result
results_summary_rf <- hotel_results_rf %>%
  collect_metrics(summarize = FALSE) %>%
  select(c(".metric", ".estimate")) %>%
  group_by(.metric) %>%
  summarise(variance = var(.estimate))
results_summary_rf
```

Again, we do not see clear signs of overfitting. Let's proceed to perform the same evaluation we did on the decision tree model.

```{r}
rf_fit_train <- fit(rf_wf, data = hotel_train)
rf_test <- augment(rf_fit_train,
                                hotel_test) %>%
  select(booking_status, starts_with(".pred"))

predictions <- rf_test %>%
  select(booking_status, .pred_class, .pred_Canceled)

sensitivity_val <- predictions %>% sens(truth = booking_status, estimate = .pred_class)
specificity_val <- predictions %>% spec(truth = booking_status, estimate = .pred_class)
accuracy_val <- predictions %>% accuracy(truth = booking_status, estimate = .pred_class)
roc_auc_val <- predictions %>% roc_auc(truth = booking_status, .pred_Canceled)

model_metrics <- tibble(
  .metric = c("accuracy", "roc_auc", "sensitivity", "specificity"),
  test = c(accuracy_val$.estimate, roc_auc_val$.estimate, sensitivity_val$.estimate, specificity_val$.estimate)
)
model_metrics
```

Notice a decent improvement in all four metrics compared to the decision tree model. This supports and demonstrates the idea that an ensemble model makes better predictions than a single model.

::: callout-important
#### Action

Try altering the number of trees or the max depths of the trees to see if the results change. Can you achieve a better model performance?
:::

## Hyper-parameters Tuning

As you notice in the last section, a few adjustments in the model parameters may result in big differences in its final performance.

In a random forest model, there is a couple of important hyper-parameters to tune:

<details>

<summary>Overview of some common hyper-paremeters</summary>

Number of Variables per split (`mtry` in `rand_forest()`): This is the number of variables considered for splitting at each node. Higher values are more likely to result in overfitting.

Number of Trees (`trees` in `rand_forest()`): This refers to the number of trees in the forest. Generally, more trees increase model performance and robustness but also raise computational costs.

Maximum Depth of Trees (`max_depth` in the `ranger` engine): This sets the maximum depth of each tree. Deeper trees can model complex patterns but are also prone to overfitting. If there is a minimum number of samples required to split a node, you may set this to None.

Minimum Samples Split (`min_n` in `rand_forest()`): This is the minimum number of samples required to split an internal node. Higher values can prevent creating nodes that might overfit your data.

Criterion (`importance` in the `ranger` engine): his function measures the quality of a split. Supported criteria include "impurity" for the Gini impurity and "none" for information gain.

Class Weight (`class.weight` in the `ranger` engine): This parameter is used to balance the dataset and is particularly important in dealing with imbalanced datasets.

</details>

In this section we will learn how to use grid to search for the best parameter combination. For simplicity we only focus on three parameters: `mtry`, `trees`, and `min_n`.

First we need to create another model using `tune()` as placeholders for parameter values.

```{r}
# Define the model (Random Forest)
rf_model_tune <- rand_forest(mtry = tune(), ## number of random sampled predictors used for each split
                             trees = tune(), ## number of trees
                             min_n = tune()) %>% ## minimum number of data points needed in a node to split
  set_mode("classification") %>% ## type of tasks
  set_engine("ranger", importance = "impurity") ## type of engine used to fit the model

```

Next, create a grid that specifies the range of values each parameter may hold. `levels` indicates how many values from the range will be selected. The selected values would divide each range into equal intervals.

For example, a grid with three tuning parameters and three levels will have 3\^3 combinations of parameter lists. 3\^3 models will be fitted, each corresponding to a combination of parameters.

```{r}
rf_grid <- grid_regular(
  mtry(c(5, 16)),
  trees(c(50, 100)),
  min_n(c(10, 20)),
  levels = 3
)
```

Create a workflow as usual, but use `tune_grid(..., grid = )` to fit the model this time.

```{r}
# combine the model and the dataset to a workflow
rf_wf_tune <- workflow() %>% 
  add_model(rf_model_tune) %>% 
  add_recipe(hotel_recipe)

hotel_results_rf_tune <- tune_grid(
  rf_wf_tune,
  resamples = hotel_folds,
  grid = rf_grid)
```

`show_best()` will show the best models out of all 27 and their performance metrics.

```{r}
show_best(hotel_results_rf_tune, metric = "roc_auc")
```

The `std_err` column shows us that there is no sign of overfitting.

Before we proceed to augment the testing set, first, we need to select the best model out of all 27 models, and finalize the workflow with the best parameter list.

```{r}
best_params <- select_best(hotel_results_rf_tune, metric = "accuracy") 
rf_final_workflow_train <- finalize_workflow(rf_wf_tune, best_params) 
```

Now we can repeat the evaluation process

```{r}
rf_final_fit_train <- fit(rf_final_workflow_train, data = hotel_train) 
rf_final_test <- augment(rf_final_fit_train, hotel_test) %>% 
  select(booking_status, starts_with(".pred"))

predictions <- rf_final_test %>% 
  select(booking_status, .pred_class, .pred_Canceled)

sensitivity_val <- predictions %>% sens(truth = booking_status, estimate = .pred_class) 
specificity_val <- predictions %>% spec(truth = booking_status, estimate = .pred_class)
accuracy_val <- predictions %>% accuracy(truth = booking_status, estimate = .pred_class)
roc_auc_val <- predictions %>% roc_auc(truth = booking_status, .pred_Canceled)

# Create a new variable to store the metrics

model_metrics <- tibble(
  .metric = c("accuracy", "roc_auc", "sensitivity", "specificity"),
  test = c(accuracy_val$.estimate, roc_auc_val$.estimate, sensitivity_val$.estimate, specificity_val$.estimate)
)

model_metrics
```

::: callout-important
#### Action

Try to tune other parameters of the Random Forest model using a grid to see if the results change. Is there a better model performance?
:::

## Understanding the Random Forest model

### Features Importance

Random Forest can be used to rank the importance of variables in a regression or classification problem.

The higher the value of the mean decrease in accuracy or mean decrease in Gini score, the greater the importance of the variable in the model.

A useful tool for visualizing feature importance is the VIP plot (Variable Importance Plot). You can use the vip() function from the vip package, as shown below:

```{r}
rf_final_fit_train %>% extract_fit_parsnip() %>% 
  vip() +
  theme_minimal()
```

Like we analyzed before, lead_time is the most important feature after we plot the Variance Important Plot. So our results justified our preliminary exploratory analysis.

## Final Thoughts

After reading this vignette, you should understand how Random Forest works as a machine learning algorithm and how to implement this model in R using our step-by-step instructions. You should also know how to fine-tune Random Forest by adjusting parameters such as mtry, trees, and min_n.

::: callout-important
#### Discussion

Can you try fitting Random Forest to other datasets? How does it perform with regression? Can you encapsulate the process of fitting a Random Forest model in R in a few steps?
:::

Expecting to learn more? *Here are some excellent articles that may help you progress more in this topic:*

[Understand random forest algorithms with examples](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)[^1]

[^1]: Sruthi E R (2023, October 26). \*Understand random forest algorithms with examples (updated 2023)\*. Analytics Vidhya. \<https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/\>

[Practical Tutorial on Random Forest and Parameter Tuning in R](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/)[^2]

[^2]: Manish Saraswat (2023). \*Practical Tutorial on Random Forest and Parameter Tuning in R\* . R \<https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/\>
