---
title: "vignette.qmd"
format: html
editor: visual
date: today
author: Mindy Xu, Zoe Zhou, Amy Lyu, Jiashu Huang
---

# Random Forest Vignette

## Introduction

> In this project, we are presenting a vignette on Random Forest. If you are new to this topic, then this vignette would be a great guideline for you to have a great understanding on Random forest model, and we will also show the sample code on a dataset to predict hotel cancellation using Random Forest.

*Learning Objectives:*

-   learn the intuitions behind Random Forest models

    -   Decision trees, hyperparameters, and more

-   Implement, tune, and evaluate Random Forest models in R

Here we have a introductory video for you to better know what is Random Forest.

[Introductory Video](https://www.youtube.com/watch?v=v6VJ2RO66Ag)

## Example Data -- Hotel Reservation Data to Implement

### Data Description

This dataset is obtained from kaggle. There are 19 predictors in this data set. Booking status is the target varibale, indicating whether the reservation was canceled or not (binary: 2 values). There are 36275 observations in total. The goal of predicting hotel reservations is to forecast the number of bookings a hotel will receive for a specific date range, based on historical data and other relevant factors such as seasonality, market trends, and pricing strategies.

<details>

<summary>Data Dictionary</summary>

-   Booking_ID: unique identifier of each booking

-   no_of_adults: Number of adults

-   no_of_children: Number of Children

-   no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel

-   no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel

-   type_of_meal_plan: Type of meal plan booked by the customer

-   required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)

-   room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.

-   lead_time: Number of days between the date of booking and the arrival date

-   arrival_year: Year of arrival date

-   arrival_month: Month of arrival date

-   arrival_date: Date of the month

-   market_segment_type: Market segment designation.

-   repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)

-   no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking

-   no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking

-   avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)

-   no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)

-   booking_status: Flag indicating if the booking was canceled or not.

</details>

### Load Packages and Data

```{r results='hide', message=FALSE, warning = FALSE}
# load packages
library(tidyverse)
library(dplyr)
library(tidymodels)
library(janitor) ## clean_names()
library(ranger) ## random forest model engine

# load data
hotel <- read.csv("data/HotelReservations.csv")
```

### Preprocess the data

Before implementing the model, we need to make sure the dataset is ready for use. First, change variable names to a unified format. They will contain only "\_", numbers, and letters.

```{r}
# cleaning predictor names
hotel <- clean_names(hotel)
```

Now let's take a look at how our data is stored.

```{r}
# view data structure and display the first 8 variables
str(hotel, vec.len = 2, list.len = 8)
```

Notice how the categorical variables are stored in character variables. We need to transform them into factors before we encode them into binary variables later.

::: callout-important
#### Action

Use lapply() to turn categorical variables into factors.
:::

```{r, echo=FALSE}
# Convert characters to factors and rename factor levels
hotel<- data.frame(lapply(hotel, factor))
```

## Data Partition

Split the dataset into a training and a testing set. Use "booking_status" as the stratification variable to ensure it has equivalent proportions in two sets.

```{r}
set.seed(3435)
hotel_split <- initial_split(hotel, strata=
                                 "booking_status", prop = 0.7)
hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)
```

::: callout-important
#### Action

Check the dimension of training data and testing data to see if the data has been split correctly.
:::

## Preprocessing Recipe

Now let's create a recipe for our dataset.

```{r}
# Create a recipe for the dataset
hotel_recipe <- recipe(booking_status ~., data = hotel_train) 
```

Limit the amount of factor levels for each predictor using step_other(). Factor levels with an occurring frequency less than 0.05 would be pooled to "other".

```{r}
hotel_recipe <- hotel_recipe %>% 
  step_other(all_predictors(), threshold = 0.05)
```

Using step_dummy(), encode factor variables to multiple binary variables which correspond to each factor level.

```{r}
hotel_recipe <- hotel_recipe  %>% 
  step_dummy(all_nominal_predictors()) 
```

## Cross-Validation Setup
"""BRIEFLY DESCRIBE WHAT CROSS_VALIDATION IS AND WHY WE USE IT"

```{r}
# 10-fold Cross validation
hotel_folds <- vfold_cv(hotel_train, v = 10, strata = booking_status)
```

::: callout-important
#### Action

Try to change the folds number for the cross validation to see if the results change.
:::

## Intuition and Architecture

### Understanding Decision Trees

""" DESCRIPTION OF DEICION TREES - TO BE COMPLETED"""

Now let's implement a basic decision tree. decision_tree() defines a model as a set of if/then statements that creates a tree-based structure. - set_mode() specifies the type of problem we are handling - set_engine() specifies the type of package/system used to fit the model later - set_args() specifies the values of arguments of decition_tree(); here we set the depth of the tree to be in the range of \[1, 15\].

```{r}
# Define the model (Decision Tree)
dt_model<-decision_tree() %>% 
  set_mode("classification") %>% ## type of tasks
  set_engine("rpart") %>% ## type of engine used to fit the model
  set_args(tree_depth(c(1L, 15L))) ## Other parameters
```

Create a workflow object, set its recipe to hotel_recipe, and add dt_model as its model.

```{r}
# combine the model and the dataset to a workflow
dt_wf <- workflow() %>%
  add_recipe(hotel_recipe) %>%
  add_model(dt_model)
```

Use fit_resamples() to fit multiple models on the cross-validation folds we created earlier. - use "metrics =" to specify what to keep track for each model's performance evaluation.

```{r}
# fit the model
hotel_results_dt <- fit_resamples(
  dt_wf,
  resamples = hotel_folds, ## cross-validation
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)
```

Let's see how this model performed.

```{r}
# summarize the result
results_summary_dt <- hotel_results_dt %>%
  collect_metrics() %>%
  select(c(".metric", "mean"))
results_summary_dt
```

::: callout-important
#### Action

Play around with different parameters (engine, tree_depth, number of folds, etc) to see if you can achieve a higher performance.
:::

### Random Forest and Ensemble models
"""DESCRIPTION OF RANDOM FOREST - TO BE COMPLETED"

Use rand_forest() to implement a random forest model. Similar to the decision tree model we implemented earlier, use set_mode() and set_engine() to complete our model definition.

There is a couple of parameters you can play around with:
- mtry: the number of random sampled predictors used for each split
- trees: number of trees
- min_n: minimum number of data needed in a node to split further into two branches.
```{r}
rf_model <- rand_forest(mtry = 15,
                        trees = 20, 
                        min_n = 10) %>% 
  set_mode("classification") %>% ## type of tasks
  set_engine("ranger") ## type of engine used to fit the model
```
Again, combine the model and recipe into a workflow.
```{r}
rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(hotel_recipe)
```
Fit the model using fit_resamples(). You will notice this time it takes longer for the model to fit.
```{r}
# fit the model
hotel_results_rf <- fit_resamples(
  rf_wf,
  resamples = hotel_folds,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)

```

::: {.callout-important style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try altering the number of trees or the max depths of the trees to see if the results change. Is there a better model performance?
:::

### Result of Random Forest Model

```{r}
# summarize the result
results_summary_rf <- hotel_results_rf %>%
  collect_metrics() %>%
  select(c(".metric", "mean"))
results_summary_rf
```

## Hyperparameters Tuning

### Random Forest Parameters overview

```{r}
# Define the model (Random Forest)
rf_model_tune <- rand_forest(mtry = tune(), ## number of random sampled predictors used for each split
                             trees = tune(), ## number of trees
                             min_n = tune()) %>% ## minimum number of data points needed in a node to split
  set_mode("classification") %>% ## type of tasks
  set_engine("ranger") ## type of engine used to fit the model

```

### Grid/Cross-validation

```{r}
rf_grid <- grid_regular(
  mtry(c(2, 18)),
  trees(c(10, 200)),
  min_n(c(10, 50)),
  levels = 3
)

# combine the model and the dataset to a workflow
rf_wf_tune <- workflow() %>% 
  add_model(rf_model_tune) %>% 
  add_recipe(hotel_recipe)

#hotel_results_rf_tune <- tune_grid(
#  rf_wf_tune,
#  resamples = hotel_folds,
#  grid = rf_grid)

#show_best(hotel_results_rf_tune, metric = "roc_auc")
```

::: {.callout-important style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try to tune other parameters of the Random Forest model using a grid to see if the results change. Is there a better model performance?
:::

## Understanding the Random Forest model

### Features Importance

\`\`\`

/\* Description /\* Sample code \`\`\`

> Random Forest Algorithm widespread popularity stems from its user-friendly nature and adaptability, enabling it to tackle both classification and regression problems effectively. The algorithm's strength lies in its ability to handle complex datasets and mitigate overfitting, making it a valuable tool for various predictive tasks in machine learning.

> One of the most important features of the Random Forest Algorithm is that it can handle the data set containing **continuous variables**, as in the case of **regression**, and **categorical variables**, as in the case of **classification**. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.

![](image/img-rfsimplified.png)

#### Steps Involved in Random Forest Algorithm

-   Step 1：In the Random forest model, a subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.
-   Step 2：Individual decision trees are constructed for each sample.
-   Step 3：Each decision tree will generate an output.
-   Step 4：Final output is considered based on ***Majority Voting or Averaging*** for Classification and regression, respectively.

#### Important Features of Random Forest

-   **Ensemble of Decision Trees：** Random Forest is an ensemble learning method, which means it combines the predictions from multiple machine learning algorithms to make more accurate predictions than any individual model. Specifically, it builds multiple decision trees and merges them together to get a more accurate and stable prediction.

-   **Handling of Both Categorical and Numerical Features：**It can handle a mix of categorical and numerical features. There is no need to pre-process data to convert categorical features to numerical features.

-   **Feature Importance：**One of the useful outputs of Random Forest is the importance or contribution of each feature in the prediction. This helps in understanding the data better and can be used for feature selection.

-   **Avoids Overfitting：** Due to the way it constructs the decision trees (using a subset of features and samples), it generally does a good job of avoiding overfitting, especially compared to individual decision trees.

-   **No Need for Future Scaling：**Random Forest does not require feature scaling (like standardization or normalization) before input, as it does not rely on distance calculations.

-   **Handles Missing Values:** It can handle missing values in the data, though the way it does this can vary depending on the implementation.

-   **Robust to Outliers:** It is generally robust to outliers and can handle them better than many other algorithms.

-   **Good for Large Datasets：**It can handle large datasets with higher dimensionality (many features) and can evaluate the importance of different features for the classification/regression tasks.

-   **Versatility in Performance Metrics：** It supports various metrics for evaluating the performance suitable for different types of problems (classification, regression)

-   **Parallelizable：** The algorithm can be parallelized for execution because each tree in the forest is built independently of the others, which makes the algorithm well-suited for modern multi-processor computers.
