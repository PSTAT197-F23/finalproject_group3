---
title: "vignette.qmd"
format: html
editor: visual
date: today
author: Mindy Xu, Zoe Zhou, Amy Lyu, Jerry Huang
---

# Random Forest Vignette

## Introduction

> In this project, we are presenting a vignette on Random Forest. If you are new to this topic, then this vignette would be a great guideline for you to have a great understanding on Random forest model, and we will also show the sample code on a dataset to predict hotel cancellation using Random Forest.

*Learning Objectives:*

-   learn the intuitions behind Random Forest models

    -   Decision trees, hyperparameters, and more

-   Implement, tune, and evaluate Random Forest models in R

Here we have a introductory video for you to better know what is Random Forest.

[Introductory Video](https://www.youtube.com/watch?v=v6VJ2RO66Ag)

## Example Data -- Hotel Reservation Data to Implement

### Data Description

```         
This dataset is obtained from kaggle. There are 19 predictors in this data set. Booking status is the target varibale, indicating whether the reservation was canceled or not (binary: 2 values). There are 36275 observations in total. The goal of predicting hotel reservations is to forecast the number of bookings a hotel will receive for a specific date range, based on historical data and other relevant factors such as seasonality, market trends, and pricing strategies.
```

#### Data Dictionary

-   Booking_ID: unique identifier of each booking

-   no_of_adults: Number of adults

-   no_of_children: Number of Children

-   no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel

-   no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel

-   type_of_meal_plan: Type of meal plan booked by the customer

-   required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)

-   room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.

-   lead_time: Number of days between the date of booking and the arrival date

-   arrival_year: Year of arrival date

-   arrival_month: Month of arrival date

-   arrival_date: Date of the month

-   market_segment_type: Market segment designation.

-   repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)

-   no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking

-   no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking

-   avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)

-   no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)

-   booking_status: Flag indicating if the booking was canceled or not.

### Load Packages and data

```{r results='hide', message=FALSE}
# loading packages
library(tidyverse)
library(dplyr)
library(tidymodels)
library(janitor) ## clean_names()
library(ranger) ## random forest model engine

# loading the data
hotel <- read.csv("data/HotelReservations.csv")
```

### Preprocess the data

```{r}
# cleaning predictor names
hotel <- clean_names(hotel)

# summarize data
summary(hotel) # Categorical variables are stored in "character" variables

# Convert characters to factors and rename factor levels
hotel<- data.frame(lapply(hotel, factor))
summary(hotel)
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Turn other categorical variables into factors.
:::

## Data Partition

```{r}
set.seed(3435)
hotel_split <- initial_split(hotel, strata=
                                 "booking_status", prop = 0.7)
hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try split the data with different stratification to see if the results change. Is there a better stratification? Check the dimension of training data and testing data to see if the data has been split correctly.
:::

## Preprocessing Recipe

```{r}
# Create a recipe for the dataset
hotel_recipe <- 
  recipe(booking_status ~., data = hotel_train) %>% 
  step_other(all_predictors(), threshold = 0.05) %>% ## factor levels with an occurring frequency less than 0.05 would be pooled to "other"
  step_dummy(all_nominal_predictors()) ## factors will be encoded to multiple binary variables corresponding to each level
```

## Cross-Valdation Setup

```{r}
# 10-fold Cross validation
hotel_folds <- vfold_cv(hotel_train, v = 10, strata = booking_status)
```

#### Action

Try to change the folds number for the cross validation to see if the results change. :::

## Intuition and Architecture

### Understanding Decision Trees

```{r}
# Define the model (Decision Tree)
dt_model<-decision_tree() %>% 
  set_mode("classification") %>% ## type of tasks
  set_engine("rpart") %>% ## type of engine used to fit the model
  set_args(tree_depth(c(1L, 15L))) ## Other parameters

# combine the model and the dataset to a workflow
dt_wf <- workflow() %>%
  add_recipe(hotel_recipe) %>%
  add_model(dt_model)

# fit the model
hotel_results_dt <- fit_resamples(
  dt_wf,
  resamples = hotel_folds, ## cross-validation
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Action for decision tree
:::

### Result for Decision Tree Model

```{r}
# summarize the result
results_summary_dt <- hotel_results_dt %>%
  collect_metrics() %>%
  select(c(".metric", "mean"))
results_summary_dt
```

### Random Forest and Ensemble models

```{r}
# Define the model (Random Forest)
rf_model <- rand_forest(mtry = 15, ## number of random sampled predictors used for each split
                        trees = 20, ## number of trees
                        min_n = 10) %>% ## minimum number of data points needed in a node to split
  set_mode("classification") %>% ## type of tasks
  set_engine("ranger") ## type of engine used to fit the model

# combine the model and the dataset to a workflow
rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(hotel_recipe)

# fit the model
hotel_results_rf <- fit_resamples(
  rf_wf,
  resamples = hotel_folds, ## cross-validation
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity) ## metrics to keep track on
)

```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try altering the number of trees or the max depths of the trees to see if the results change. Is there a better model performance?
:::

### Result of Random Forest Model

```{r}
# summarize the result
results_summary_rf <- hotel_results_rf %>%
  collect_metrics() %>%
  select(c(".metric", "mean"))
results_summary_rf
```

## Hyperparameters Tuning

### Random Forest Parameters overview

```{r}
# Define the model (Random Forest)
rf_model_tune <- rand_forest(mtry = tune(), ## number of random sampled predictors used for each split
                             trees = tune(), ## number of trees
                             min_n = tune()) %>% ## minimum number of data points needed in a node to split
  set_mode("classification") %>% ## type of tasks
  set_engine("ranger") ## type of engine used to fit the model

```

### Grid/Cross-validation

```{r}
rf_grid <- grid_regular(
  mtry(c(2, 18)),
  trees(c(10, 200)),
  min_n(c(10, 50)),
  levels = 3
)

# combine the model and the dataset to a workflow
rf_wf_tune <- workflow() %>% 
  add_model(rf_model_tune) %>% 
  add_recipe(hotel_recipe)

hotel_results_rf_tune <- tune_grid(
  rf_wf_tune,
  resamples = hotel_folds,
  grid = rf_grid)

show_best(hotel_results_rf_tune, metric = "roc_auc")
```

::: {.callout-note style="background-color: #ffebee; color: black; border-left: 3px solid red;"}
#### Action

Try to tune other parameters of the Random Forest model using a grid to see if the results change. Is there a better model performance?
:::

## Understanding the Random Forest model

### Features Importance

\`\`\`

/\* Description /\* Sample code \`\`\`

> Random Forest Algorithm widespread popularity stems from its user-friendly nature and adaptability, enabling it to tackle both classification and regression problems effectively. The algorithm's strength lies in its ability to handle complex datasets and mitigate overfitting, making it a valuable tool for various predictive tasks in machine learning.

> One of the most important features of the Random Forest Algorithm is that it can handle the data set containing **continuous variables**, as in the case of **regression**, and **categorical variables**, as in the case of **classification**. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.

![](image/img-rfsimplified.png)

#### Steps Involved in Random Forest Algorithm

-   Step 1：In the Random forest model, a subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.
-   Step 2：Individual decision trees are constructed for each sample.
-   Step 3：Each decision tree will generate an output.
-   Step 4：Final output is considered based on ***Majority Voting or Averaging*** for Classification and regression, respectively.

#### Important Features of Random Forest

-   **Ensemble of Decision Trees：** Random Forest is an ensemble learning method, which means it combines the predictions from multiple machine learning algorithms to make more accurate predictions than any individual model. Specifically, it builds multiple decision trees and merges them together to get a more accurate and stable prediction.

-   **Handling of Both Categorical and Numerical Features：**It can handle a mix of categorical and numerical features. There is no need to pre-process data to convert categorical features to numerical features.

-   **Feature Importance：**One of the useful outputs of Random Forest is the importance or contribution of each feature in the prediction. This helps in understanding the data better and can be used for feature selection.

-   **Avoids Overfitting：** Due to the way it constructs the decision trees (using a subset of features and samples), it generally does a good job of avoiding overfitting, especially compared to individual decision trees.

-   **No Need for Future Scaling：**Random Forest does not require feature scaling (like standardization or normalization) before input, as it does not rely on distance calculations.

-   **Handles Missing Values:** It can handle missing values in the data, though the way it does this can vary depending on the implementation.

-   **Robust to Outliers:** It is generally robust to outliers and can handle them better than many other algorithms.

-   **Good for Large Datasets：**It can handle large datasets with higher dimensionality (many features) and can evaluate the importance of different features for the classification/regression tasks.

-   **Versatility in Performance Metrics：** It supports various metrics for evaluating the performance suitable for different types of problems (classification, regression)

-   **Parallelizable：** The algorithm can be parallelized for execution because each tree in the forest is built independently of the others, which makes the algorithm well-suited for modern multi-processor computers.
